{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jamey\\Developments\\DS-Project\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import warnings\n",
    "from machine_learning.utils import split_data, scale_dataset, get_distribution, plot_distribution\n",
    "from machine_learning.neural_networks.utils import plot_history\n",
    "from machine_learning.neural_networks.shallow_fnn import train_shallow_fnn_model\n",
    "from configs.enums import Column\n",
    "import numpy as np\n",
    "from configs.data import MACHINE_LEARNING_DATASET_PATH, MODELS_PATH, VERSION\n",
    "import shap\n",
    "import tensorflow as tf\n",
    "import machine_learning.neural_networks.learning_rate_schedulers as lrs\n",
    "from typing import Tuple\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "np.random.seed(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T16:55:12.609311300Z",
     "start_time": "2024-01-05T16:55:04.184054500Z"
    }
   },
   "id": "bd7b15f8c001f56e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8d5bd14682ded87"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (10836, 9), Valid: (1494, 9), Test: (1499, 9)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(MACHINE_LEARNING_DATASET_PATH)\n",
    "train_df, valid_df, test_df = split_data(df)\n",
    "train, x_train, train_labels = scale_dataset(train_df, oversample=True)\n",
    "valid, x_val, val_labels = scale_dataset(valid_df, oversample=False)\n",
    "test, x_test, test_labels = scale_dataset(test_df, oversample=False)\n",
    "\n",
    "print(f\"Train: {x_train.shape}, Valid: {x_val.shape}, Test: {x_test.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T16:55:13.334591600Z",
     "start_time": "2024-01-05T16:55:12.613314200Z"
    }
   },
   "id": "a0e065b98cfdeb40"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utility function definitions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d26d47f094632c3a"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def print_results(model) -> Tuple:\n",
    "    y_pred_train = model.predict(x_train).argmax(axis=1)\n",
    "    print(\"\\n###### Training ######\")\n",
    "    print(classification_report(train_labels, y_pred_train))\n",
    "    \n",
    "    y_pred_valid = model.predict(x_val).argmax(axis=1)\n",
    "    print(\"\\n###### Validation ######\")\n",
    "    print(classification_report(val_labels, y_pred_valid))\n",
    "    \n",
    "    y_pred = model.predict(x_test).argmax(axis=1)\n",
    "    print(\"\\n###### Test ######\")\n",
    "    print(classification_report(test_labels, y_pred))\n",
    "    \n",
    "    return y_pred_train, y_pred_valid, y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T16:55:13.336833Z",
     "start_time": "2024-01-05T16:55:13.329934800Z"
    }
   },
   "id": "3037effd47510c0e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load model from file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e24fea2893989d07"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339/339 [==============================] - 0s 989us/step\n",
      "\n",
      "###### Training ######\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1204\n",
      "           1       1.00      0.98      0.99      1204\n",
      "           2       0.98      0.97      0.98      1204\n",
      "           3       0.96      0.99      0.97      1204\n",
      "           4       0.95      0.93      0.94      1204\n",
      "           5       0.92      0.89      0.91      1204\n",
      "           6       0.93      0.90      0.91      1204\n",
      "           7       0.94      0.97      0.96      1204\n",
      "           8       0.96      1.00      0.98      1204\n",
      "\n",
      "    accuracy                           0.96     10836\n",
      "   macro avg       0.96      0.96      0.96     10836\n",
      "weighted avg       0.96      0.96      0.96     10836\n",
      "\n",
      "47/47 [==============================] - 0s 808us/step\n",
      "\n",
      "###### Validation ######\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92        17\n",
      "           1       0.99      0.92      0.96        92\n",
      "           2       0.95      0.94      0.94       190\n",
      "           3       0.90      0.96      0.93       318\n",
      "           4       0.85      0.93      0.89       401\n",
      "           5       0.93      0.83      0.87       358\n",
      "           6       0.94      0.75      0.83        96\n",
      "           7       0.84      0.94      0.89        17\n",
      "           8       0.83      1.00      0.91         5\n",
      "\n",
      "    accuracy                           0.90      1494\n",
      "   macro avg       0.90      0.92      0.90      1494\n",
      "weighted avg       0.91      0.90      0.90      1494\n",
      "\n",
      "47/47 [==============================] - 0s 705us/step\n",
      "\n",
      "###### Test ######\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97        17\n",
      "           1       0.99      0.98      0.98        93\n",
      "           2       0.98      0.96      0.97       190\n",
      "           3       0.93      0.98      0.96       319\n",
      "           4       0.93      0.94      0.94       402\n",
      "           5       0.93      0.91      0.92       358\n",
      "           6       0.90      0.74      0.81        96\n",
      "           7       0.62      1.00      0.77        18\n",
      "           8       1.00      0.67      0.80         6\n",
      "\n",
      "    accuracy                           0.93      1499\n",
      "   macro avg       0.91      0.91      0.90      1499\n",
      "weighted avg       0.94      0.93      0.93      1499\n",
      "\n",
      "47/47 [==============================] - 0s 821us/step\n"
     ]
    }
   ],
   "source": [
    "model_file = \"RawData.9c_Adam_1024_0_#FactorScheduler-factor_0.995-stop_factor_0.00075-base_lr_0.00075#_200_25_32_0.19385148584842682.shallow_fnn.keras\"\n",
    "model = tf.keras.models.load_model(os.path.join(MODELS_PATH, \"1e-07_09_0999_0_None_None\", model_file))\n",
    "\n",
    "_, _, y_pred = print_results(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T16:55:57.108395600Z",
     "start_time": "2024-01-05T16:55:56.175003800Z"
    }
   },
   "id": "f733c245218b44aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ced6d476f2df20d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parameter tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a853047f843560eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tune_model(df,\n",
    "               units,\n",
    "               dropout_rates,\n",
    "               learning_rates,\n",
    "               epochs=200,\n",
    "               patience=[10, 20],\n",
    "               batch_sizes=[128]):\n",
    "    import time\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    least_val_lost_file_name = f\"tuning_least_val_loss.fnn.keras\"\n",
    "    \n",
    "    to_hh_mm_ss = lambda seconds: str(timedelta(seconds=seconds)).rsplit(\".\")[0]\n",
    "    \n",
    "    least_val_loss = float('inf')\n",
    "    least_val_loss_params = []\n",
    "    least_val_loss_accuracy = float('inf')\n",
    "    least_val_loss_model = None  \n",
    "    least_val_loss_history = None\n",
    "    eta = None\n",
    "    \n",
    "    time_past = 0\n",
    "    \n",
    "    i = 1 \n",
    "    max = len(units) * len(dropout_rates) * len(learning_rates) * len(patience) * len(batch_sizes)\n",
    "    \n",
    "    print(\"[prev: N/A] [eta: TBD]\")\n",
    "    \n",
    "    for u in units:\n",
    "        for dr in dropout_rates:\n",
    "            for lr in learning_rates: \n",
    "                for pt in patience:\n",
    "                    for bt in batch_sizes:\n",
    "                        start_time = time.time()\n",
    "\n",
    "                        print(f\"[{i}/{max}] Units: {u}; Dropout rate: {dr}; Learning rate: {lr}; Patience: {pt}; Batch size: {bt}\")\n",
    "                        \n",
    "                        # TODO: add batch_size param to tune\n",
    "                        model, history, num_classes = train_shallow_fnn_model(\n",
    "                            df, \n",
    "                            epochs=epochs, \n",
    "                            patience=pt, \n",
    "                            units=u,\n",
    "                            dropout_rate=dr,\n",
    "                            learning_rate=lr,\n",
    "                            epsilon = 1e-07,\n",
    "                            beta_1 = 0.9,\n",
    "                            beta_2 = 0.999,\n",
    "                            weight_decay = 0,\n",
    "                            clipnorm = None,\n",
    "                            clipvalue = None,\n",
    "                            batch_size=bt,\n",
    "                            verbose=0,\n",
    "                            disable_save=True,\n",
    "                            disable_plot_history=True,\n",
    "                            disable_print_report=True)\n",
    "                        \n",
    "                        val_loss, val_acc = model.evaluate(x_test, test_labels)\n",
    "                        print(f\"Loss: {val_loss}; Accuracy: {val_acc};\")\n",
    "                        if val_loss < least_val_loss:\n",
    "                            model.save(os.path.join(MODELS_PATH, least_val_lost_file_name))\n",
    "                            least_val_loss = val_loss\n",
    "                            least_val_loss_params = [ u, dr, lr, pt, bt]\n",
    "                            least_val_loss_accuracy = val_acc\n",
    "                            least_val_loss_model = model\n",
    "                            least_val_loss_history = history\n",
    "                            \n",
    "                        duration = time.time() - start_time\n",
    "                        time_past += duration\n",
    "                        avg_duration = time_past / i\n",
    "                        eta = time_past + avg_duration * (max - i)\n",
    "                        \n",
    "                        print(f\"\\n[eta: {to_hh_mm_ss(time_past)}/{to_hh_mm_ss(eta)}] [prev: {to_hh_mm_ss(duration)}] [avg: {to_hh_mm_ss(avg_duration)}]\")\n",
    "                            \n",
    "                        i += 1\n",
    "    \n",
    "    u, dr, lr, pt, bt = least_val_loss_params\n",
    "    print(\"\\nLeast validation loss:\")              \n",
    "    print(f\"\\tParams:\\t {{Units: {u}; Dropout rate: {dr}; Learning rate: {lr}; Patience: {pt}; Batch size: {bt}}}\")\n",
    "    print(\"\\tLoss:\\t\", least_val_loss)\n",
    "    print(\"\\tAccuracy:\\t\", least_val_loss_accuracy)\n",
    "    \n",
    "    best_model_file_name = f\"{VERSION}_Adam_{u}_{dr}_{lr}_{epochs}_{pt}_{bt}_{least_val_loss}.shallow_fnn.keras\"\n",
    "    os.rename(\n",
    "        os.path.join(MODELS_PATH, least_val_lost_file_name), \n",
    "        os.path.join(MODELS_PATH, best_model_file_name))\n",
    "    print(f\"\\nModel has been saved as '{best_model_file_name}'\")\n",
    "    \n",
    "    plot_history(least_val_loss_history, num_classes)\n",
    "    \n",
    "    print_results(least_val_loss_model)\n",
    "    \n",
    "    return least_val_loss_model, least_val_loss_history, num_classes\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model, history, _ = tune_model(\n",
    "    df=df,\n",
    "    units=[512, 768, 1024],\n",
    "    dropout_rates=[0],\n",
    "    learning_rates=[\n",
    "        0.0015, \n",
    "        0.00175, \n",
    "        lrs.FactorScheduler(factor=0.995, stop_factor=0.00075, base_lr=0.002), \n",
    "        lrs.FactorScheduler(factor=1.005, stop_factor=0.002, base_lr=0.00075)],\n",
    "    patience=[10, 20, 25, 30], \n",
    "    batch_sizes=[32, 64, 128])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b08d39c0a5a3d8dd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7d03e693bf977d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tune_hyperparameters(df,\n",
    "                         learning_rates,\n",
    "                         epsilons = [1e-07],\n",
    "                         beta_1s = [0.9],\n",
    "                         beta_2s = [0.999],\n",
    "                         weight_decay = [None],\n",
    "                         clipnorm = [None],\n",
    "                         clipvalue = [None],\n",
    "                         patience=[10, 20]):\n",
    "    import time\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    least_val_lost_file_name = f\"tuning_least_val_loss.fnn.keras\"\n",
    "    \n",
    "    to_hh_mm_ss = lambda seconds: str(timedelta(seconds=seconds)).rsplit(\".\")[0]\n",
    "    \n",
    "    least_val_loss = float('inf')\n",
    "    least_val_loss_params = []\n",
    "    least_val_loss_accuracy = float('inf')\n",
    "    least_val_loss_model = None  \n",
    "    least_val_loss_history = None\n",
    "    eta = None\n",
    "    \n",
    "    time_past = 0\n",
    "    \n",
    "    i = 1 \n",
    "    max = len(epsilons) * len(beta_1s) * len(beta_2s) * len(learning_rates) * len(weight_decay) * len(clipnorm) * len(clipvalue) * len(patience)\n",
    "    \n",
    "    print(\"[prev: N/A] [eta: TBD]\")\n",
    "    \n",
    "    for e in epsilons:\n",
    "        for b1 in beta_1s:\n",
    "            for b2 in beta_2s: \n",
    "                for lr in learning_rates: \n",
    "                    for wd in weight_decay:\n",
    "                        for cn in clipnorm:\n",
    "                            for cv in clipvalue:\n",
    "                                for pt in patience:\n",
    "                                    start_time = time.time()\n",
    "            \n",
    "                                    print(f\"[{i}/{max}] Epsilons: {e}; Beta 1: {b1}; Beta 2: {b2}; Learning rate: {lr}; Weight decay: {wd}; Clipnorm: {cn}; Clipvalue: {cv}; Patience: {pt}\")\n",
    "                                    \n",
    "                                    # TODO: add batch_size param to tune\n",
    "                                    model, history, num_classes = train_shallow_fnn_model(\n",
    "                                        df, \n",
    "                                        epochs=1000, \n",
    "                                        patience=pt, \n",
    "                                        units=1024,\n",
    "                                        dropout_rate=0,\n",
    "                                        learning_rate=lr,\n",
    "                                        epsilon = e,\n",
    "                                        beta_1 = b1,\n",
    "                                        beta_2 = b2,\n",
    "                                        weight_decay = wd,\n",
    "                                        clipnorm = cn,\n",
    "                                        clipvalue = cv,\n",
    "                                        batch_size=32,\n",
    "                                        verbose=0,\n",
    "                                        disable_save=True,\n",
    "                                        disable_plot_history=True,\n",
    "                                        disable_print_report=True)\n",
    "                                    \n",
    "                                    val_loss, val_acc = model.evaluate(x_test, test_labels)\n",
    "                                    print(f\"Loss: {val_loss}; Accuracy: {val_acc};\")\n",
    "                                    if val_loss < least_val_loss:\n",
    "                                        model.save(os.path.join(MODELS_PATH, least_val_lost_file_name))\n",
    "                                        least_val_loss = val_loss\n",
    "                                        least_val_loss_params = [e, b1, b2, lr, wd, cn, cv, pt]\n",
    "                                        least_val_loss_accuracy = val_acc\n",
    "                                        least_val_loss_model = model\n",
    "                                        least_val_loss_history = history\n",
    "                                        \n",
    "                                    duration = time.time() - start_time\n",
    "                                    time_past += duration\n",
    "                                    avg_duration = time_past / i\n",
    "                                    eta = time_past + avg_duration * (max - i)\n",
    "                                    \n",
    "                                    print(f\"\\n[eta: {to_hh_mm_ss(time_past)}/{to_hh_mm_ss(eta)}] [prev: {to_hh_mm_ss(duration)}] [avg: {to_hh_mm_ss(avg_duration)}]\")\n",
    "                                        \n",
    "                                    i += 1\n",
    "    \n",
    "    e, b1, b2, lr, wd, cn, cv, pt = least_val_loss_params\n",
    "    print(\"\\nLeast validation loss:\")              \n",
    "    print(f\"\\tParams:\\t {{Epsilons: {e}; Beta 1: {b1}; Beta 2: {b2}; Learning rate: {lr}; Weight decay: {wd}; Clipnorm: {cn}; Clipvalue: {cv}; Patience: {pt}}}\")\n",
    "    print(\"\\tLoss:\\t\", least_val_loss)\n",
    "    print(\"\\tAccuracy:\\t\", least_val_loss_accuracy)\n",
    "    \n",
    "    best_model_file_name = f\"{VERSION}_Adam_hyper_{e}_{b1}_{b2}_{lr}_{wd}_{cn}_{cv}_{pt}_{least_val_loss}.shallow_fnn.keras\"\n",
    "    os.rename(\n",
    "        os.path.join(MODELS_PATH, least_val_lost_file_name), \n",
    "        os.path.join(MODELS_PATH, best_model_file_name))\n",
    "    print(f\"\\nModel has been saved as '{best_model_file_name}'\")\n",
    "    \n",
    "    plot_history(least_val_loss_history, num_classes)\n",
    "    \n",
    "    print_results(least_val_loss_model)\n",
    "    \n",
    "    return least_val_loss_model, least_val_loss_history, num_classes\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78b70756c00f21ca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model, history, _ = tune_hyperparameters(\n",
    "    df=df,\n",
    "    learning_rates=[lrs.FactorScheduler(factor=0.995, stop_factor=0.00075, base_lr=0.002), ], \n",
    "    epsilons=[1e-06, 1e-07, 1e-08],\n",
    "    beta_1s=[0.4, 0.8, 0.9], # CANNOT BE EQUAL OR HIGHER THAN 1\n",
    "    beta_2s=[0.9, 0.999], # CANNOT BE EQUAL OR HIGHER THAN 1\n",
    "    weight_decay=[None, 0.01],\n",
    "    clipnorm=[None],\n",
    "    clipvalue=[None],\n",
    "    patience=[25]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33d917eb98bf4f9b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Manual training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d8b3f0d6e60b4d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model, _, _ = train_shallow_fnn_model(\n",
    "                        df, \n",
    "                        epochs=2000, \n",
    "                        patience=25, \n",
    "                        units=1024,\n",
    "                        dropout_rate=0,\n",
    "                        learning_rate=lrs.FactorScheduler(factor=0.995, stop_factor=0.00075, base_lr=0.002),\n",
    "                        verbose=2,\n",
    "                        epsilon=1e-07,\n",
    "                        beta_1=0.9,\n",
    "                        beta_2=0.999,\n",
    "                        weight_decay=0,\n",
    "                        clipnorm=None,\n",
    "                        clipvalue=None,\n",
    "                        disable_print_report=True,\n",
    "                        disable_save=True)\n",
    "\n",
    "_, _, y_pred = print_results(model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5511bb76b7347f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test (y-pred) difference plotting"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e6ccf30310a93a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "distribution = get_distribution(test_df, y_pred)\n",
    "# print(distribution)\n",
    "plot_distribution(distribution)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "284d312c99d33fff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def test():\n",
    "#     import os\n",
    "#     result = test_df\n",
    "#     result[\"predicted_country_risk\"] = model.predict(x_test).argmax(axis=1)\n",
    "# \n",
    "#     incorrectly_predicted = result[result[\"country_risk\"] != result[\"predicted_country_risk\"]]\n",
    "# \n",
    "#     m_df = pd.read_excel(MERGED_DATASET_PATH)\n",
    "#     wm_df = m_df.iloc[incorrectly_predicted.index, ]\n",
    "# \n",
    "#     wm_df[\"country_risk\"] = map_label_to_name(incorrectly_predicted[\"country_risk\"])\n",
    "#     wm_df[\"predicted_country_risk\"] = map_label_to_name(incorrectly_predicted[\"predicted_country_risk\"])\n",
    "# \n",
    "#     from configs.enums import RISKCLASSIFICATIONS\n",
    "# \n",
    "#     classifications = RISKCLASSIFICATIONS.get_attributes()\n",
    "#     def percentage_from_boundary(country_risk: str, risk_score: float) -> float:\n",
    "#         classification = classifications[country_risk]\n",
    "#         lower_percentage = abs(risk_score / classification.lower_bound - 1)\n",
    "#         upper_percentage =  abs(risk_score / classification.upper_bound - 1)\n",
    "#         return min(lower_percentage, upper_percentage)\n",
    "# \n",
    "#     wm_df[\"percentage_from_boundary\"] = [percentage_from_boundary(cr, nr) for cr, nr in zip(wm_df[\"country_risk\"], wm_df[\"norm_risk\"])]\n",
    "# \n",
    "#     cols = [\"year\", \"country\"] + list(incorrectly_predicted.columns) + [\"norm_risk\", \"percentage_from_boundary\"]\n",
    "#     wm_df = wm_df[cols]\n",
    "#     wm_df[\"within_2.5%\"] = [abs(x) <= 0.025 for x in wm_df[\"percentage_from_boundary\"]]\n",
    "#     return wm_df\n",
    "# \n",
    "# t = test()\n",
    "# t[\"percentage_from_boundary\"].describe()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d867b7eb7dd7f007"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Output .xlsx of incorrectly predicted rows"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c6b8265203f99e"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": "      year      country  polity2  durable  fragment  gov_instability  \\\n360   2006      Austria       10       60         0                0   \n6479  1999  Switzerland       10      151         0                0   \n1413  1992        China       -7       43         0                0   \n1369  2006        Chile       10       17         0                3   \n320   1966      Austria       10       20         0                0   \n...    ...          ...      ...      ...       ...              ...   \n2275  1988         Fiji       -1        1         0                2   \n4440  1968         Mali       -7        8         0                1   \n938   1994      Burundi        0        0         0                4   \n6140  2016      Somalia        5        4         1               24   \n6137  2013      Somalia        5        1         1               24   \n\n         gdp_rppp   gdp_rppp_pc   igov_rppp  ipriv_rppp  ippp_rppp  \\\n360    418.962891  50668.893549   12.071430   82.985825   0.433667   \n6479   410.075012  57401.389812   14.069478   91.813637   0.000000   \n1413  2555.189941   2193.352568  443.393555   44.116718   3.155458   \n1369   299.501709  18335.445458    6.972108   54.304413   0.000000   \n320    131.539581  17964.817758    7.694175   27.424131   0.000000   \n...           ...           ...         ...         ...        ...   \n2275     5.108740   6683.595448    0.119270    0.446081   0.000000   \n4440     1.465057    245.248622    0.020099    0.145065   0.000000   \n938      5.814970   1040.913950    0.030692    0.094637   0.000000   \n6140     4.321084    302.324900    0.000000    0.000000   0.009150   \n6137     3.943239    306.807533    0.000000    0.000000   0.004483   \n\n     country_risk predicted_country_risk  norm_risk  \n360         low_1                  low_2   0.220567  \n6479        low_1                  low_0   0.114132  \n1413        low_2               medium_0   0.332900  \n1369        low_2               medium_0   0.327370  \n320         low_2               medium_0   0.328612  \n...           ...                    ...        ...  \n2275       high_0               medium_2   0.673135  \n4440       high_0               medium_2   0.677843  \n938        high_0                 high_1   0.775914  \n6140       high_2                 high_1   0.933784  \n6137       high_2                 high_1   0.947758  \n\n[99 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>country</th>\n      <th>polity2</th>\n      <th>durable</th>\n      <th>fragment</th>\n      <th>gov_instability</th>\n      <th>gdp_rppp</th>\n      <th>gdp_rppp_pc</th>\n      <th>igov_rppp</th>\n      <th>ipriv_rppp</th>\n      <th>ippp_rppp</th>\n      <th>country_risk</th>\n      <th>predicted_country_risk</th>\n      <th>norm_risk</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>360</th>\n      <td>2006</td>\n      <td>Austria</td>\n      <td>10</td>\n      <td>60</td>\n      <td>0</td>\n      <td>0</td>\n      <td>418.962891</td>\n      <td>50668.893549</td>\n      <td>12.071430</td>\n      <td>82.985825</td>\n      <td>0.433667</td>\n      <td>low_1</td>\n      <td>low_2</td>\n      <td>0.220567</td>\n    </tr>\n    <tr>\n      <th>6479</th>\n      <td>1999</td>\n      <td>Switzerland</td>\n      <td>10</td>\n      <td>151</td>\n      <td>0</td>\n      <td>0</td>\n      <td>410.075012</td>\n      <td>57401.389812</td>\n      <td>14.069478</td>\n      <td>91.813637</td>\n      <td>0.000000</td>\n      <td>low_1</td>\n      <td>low_0</td>\n      <td>0.114132</td>\n    </tr>\n    <tr>\n      <th>1413</th>\n      <td>1992</td>\n      <td>China</td>\n      <td>-7</td>\n      <td>43</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2555.189941</td>\n      <td>2193.352568</td>\n      <td>443.393555</td>\n      <td>44.116718</td>\n      <td>3.155458</td>\n      <td>low_2</td>\n      <td>medium_0</td>\n      <td>0.332900</td>\n    </tr>\n    <tr>\n      <th>1369</th>\n      <td>2006</td>\n      <td>Chile</td>\n      <td>10</td>\n      <td>17</td>\n      <td>0</td>\n      <td>3</td>\n      <td>299.501709</td>\n      <td>18335.445458</td>\n      <td>6.972108</td>\n      <td>54.304413</td>\n      <td>0.000000</td>\n      <td>low_2</td>\n      <td>medium_0</td>\n      <td>0.327370</td>\n    </tr>\n    <tr>\n      <th>320</th>\n      <td>1966</td>\n      <td>Austria</td>\n      <td>10</td>\n      <td>20</td>\n      <td>0</td>\n      <td>0</td>\n      <td>131.539581</td>\n      <td>17964.817758</td>\n      <td>7.694175</td>\n      <td>27.424131</td>\n      <td>0.000000</td>\n      <td>low_2</td>\n      <td>medium_0</td>\n      <td>0.328612</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2275</th>\n      <td>1988</td>\n      <td>Fiji</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>5.108740</td>\n      <td>6683.595448</td>\n      <td>0.119270</td>\n      <td>0.446081</td>\n      <td>0.000000</td>\n      <td>high_0</td>\n      <td>medium_2</td>\n      <td>0.673135</td>\n    </tr>\n    <tr>\n      <th>4440</th>\n      <td>1968</td>\n      <td>Mali</td>\n      <td>-7</td>\n      <td>8</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.465057</td>\n      <td>245.248622</td>\n      <td>0.020099</td>\n      <td>0.145065</td>\n      <td>0.000000</td>\n      <td>high_0</td>\n      <td>medium_2</td>\n      <td>0.677843</td>\n    </tr>\n    <tr>\n      <th>938</th>\n      <td>1994</td>\n      <td>Burundi</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>5.814970</td>\n      <td>1040.913950</td>\n      <td>0.030692</td>\n      <td>0.094637</td>\n      <td>0.000000</td>\n      <td>high_0</td>\n      <td>high_1</td>\n      <td>0.775914</td>\n    </tr>\n    <tr>\n      <th>6140</th>\n      <td>2016</td>\n      <td>Somalia</td>\n      <td>5</td>\n      <td>4</td>\n      <td>1</td>\n      <td>24</td>\n      <td>4.321084</td>\n      <td>302.324900</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.009150</td>\n      <td>high_2</td>\n      <td>high_1</td>\n      <td>0.933784</td>\n    </tr>\n    <tr>\n      <th>6137</th>\n      <td>2013</td>\n      <td>Somalia</td>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>24</td>\n      <td>3.943239</td>\n      <td>306.807533</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.004483</td>\n      <td>high_2</td>\n      <td>high_1</td>\n      <td>0.947758</td>\n    </tr>\n  </tbody>\n</table>\n<p>99 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from machine_learning.utils import output_incorrectly_predicted_xlsx\n",
    "output_incorrectly_predicted_xlsx(test_df, y_pred, \"shallow_fnn\")  "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T16:56:04.078020400Z",
     "start_time": "2024-01-05T16:56:00.377911300Z"
    }
   },
   "id": "ac84db8a422016c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Shap; Feature importance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5711f7e08f44985b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, x_train)\n",
    "shap_values = explainer.shap_values(shap.sample(x_test, 10), nsamples=100) # default of nsamples = 2 * X.shape[1] + 2048 = 2066 \n",
    "# explainer.save()\n",
    "\n",
    "from configs.enums import RISKCLASSIFICATIONS\n",
    "feature_names = df.columns.tolist()\n",
    "feature_names.remove(Column.COUNTRY_RISK)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d439d16505c7c53c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, x_test, \n",
    "                  feature_names=feature_names,\n",
    "                  class_names=RISKCLASSIFICATIONS.get_names())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf0af0cca82680a9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, x_test, \n",
    "                  feature_names=feature_names,\n",
    "                  class_names=RISKCLASSIFICATIONS.get_names(), plot_size=(20, 10))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b4286323a2e555e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
