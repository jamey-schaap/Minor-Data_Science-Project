{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import warnings\n",
    "from machine_learning.utils import split_data, scale_dataset, get_distribution, plot_distribution\n",
    "from machine_learning.neural_networks.utils import plot_history\n",
    "from machine_learning.neural_networks.shallow_fnn import train_shallow_fnn_model\n",
    "from configs.enums import Column\n",
    "import numpy as np\n",
    "from configs.data import MACHINE_LEARNING_DATASET_PATH, MODELS_PATH, VERSION\n",
    "import shap\n",
    "import tensorflow as tf\n",
    "import machine_learning.neural_networks.learning_rate_schedulers as lrs\n",
    "from typing import Tuple\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "np.random.seed(0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd7b15f8c001f56e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8d5bd14682ded87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_excel(MACHINE_LEARNING_DATASET_PATH)\n",
    "train_df, valid_df, test_df = split_data(df)\n",
    "train, x_train, train_labels = scale_dataset(train_df, oversample=True)\n",
    "valid, x_val, val_labels = scale_dataset(valid_df, oversample=False)\n",
    "test, x_test, test_labels = scale_dataset(test_df, oversample=False)\n",
    "\n",
    "print(f\"Train: {x_train.shape}, Valid: {x_val.shape}, Test: {x_test.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0e065b98cfdeb40"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utility function definitions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d26d47f094632c3a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_results(model) -> Tuple:\n",
    "    y_pred_train = model.predict(x_train).argmax(axis=1)\n",
    "    print(\"\\n###### Training ######\")\n",
    "    print(classification_report(train_labels, y_pred_train))\n",
    "    \n",
    "    y_pred_valid = model.predict(x_val).argmax(axis=1)\n",
    "    print(\"\\n###### Validation ######\")\n",
    "    print(classification_report(val_labels, y_pred_valid))\n",
    "    \n",
    "    y_pred = model.predict(x_test).argmax(axis=1)\n",
    "    print(\"\\n###### Test ######\")\n",
    "    print(classification_report(test_labels, y_pred))\n",
    "    \n",
    "    return y_pred_train, y_pred_valid, y_pred"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3037effd47510c0e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load model from file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e24fea2893989d07"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_file = \"RawData.9c_Adam_1024_0_#FactorScheduler-factor_0.995-stop_factor_0.00075-base_lr_0.00075#_200_25_32_0.19385148584842682.shallow_fnn.keras\"\n",
    "model = tf.keras.models.load_model(os.path.join(MODELS_PATH, \"1e-07_09_0999_0_None_None\", model_file))\n",
    "\n",
    "_, _, y_pred = print_results(model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f733c245218b44aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ced6d476f2df20d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parameter tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a853047f843560eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tune_model(df,\n",
    "               units,\n",
    "               dropout_rates,\n",
    "               learning_rates,\n",
    "               epochs=200,\n",
    "               patience=[10, 20],\n",
    "               batch_sizes=[128]):\n",
    "    import time\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    least_val_lost_file_name = f\"tuning_least_val_loss.fnn.keras\"\n",
    "    \n",
    "    to_hh_mm_ss = lambda seconds: str(timedelta(seconds=seconds)).rsplit(\".\")[0]\n",
    "    \n",
    "    least_val_loss = float('inf')\n",
    "    least_val_loss_params = []\n",
    "    least_val_loss_accuracy = float('inf')\n",
    "    least_val_loss_model = None  \n",
    "    least_val_loss_history = None\n",
    "    eta = None\n",
    "    \n",
    "    time_past = 0\n",
    "    \n",
    "    i = 1 \n",
    "    max = len(units) * len(dropout_rates) * len(learning_rates) * len(patience) * len(batch_sizes)\n",
    "    \n",
    "    print(\"[prev: N/A] [eta: TBD]\")\n",
    "    \n",
    "    for u in units:\n",
    "        for dr in dropout_rates:\n",
    "            for lr in learning_rates: \n",
    "                for pt in patience:\n",
    "                    for bt in batch_sizes:\n",
    "                        start_time = time.time()\n",
    "\n",
    "                        print(f\"[{i}/{max}] Units: {u}; Dropout rate: {dr}; Learning rate: {lr}; Patience: {pt}; Batch size: {bt}\")\n",
    "                        \n",
    "                        # TODO: add batch_size param to tune\n",
    "                        model, history, num_classes = train_shallow_fnn_model(\n",
    "                            df, \n",
    "                            epochs=epochs, \n",
    "                            patience=pt, \n",
    "                            units=u,\n",
    "                            dropout_rate=dr,\n",
    "                            learning_rate=lr,\n",
    "                            epsilon = 1e-07,\n",
    "                            beta_1 = 0.9,\n",
    "                            beta_2 = 0.999,\n",
    "                            weight_decay = 0,\n",
    "                            clipnorm = None,\n",
    "                            clipvalue = None,\n",
    "                            batch_size=bt,\n",
    "                            verbose=0,\n",
    "                            disable_save=True,\n",
    "                            disable_plot_history=True,\n",
    "                            disable_print_report=True)\n",
    "                        \n",
    "                        val_loss, val_acc = model.evaluate(x_test, test_labels)\n",
    "                        print(f\"Loss: {val_loss}; Accuracy: {val_acc};\")\n",
    "                        if val_loss < least_val_loss:\n",
    "                            model.save(os.path.join(MODELS_PATH, least_val_lost_file_name))\n",
    "                            least_val_loss = val_loss\n",
    "                            least_val_loss_params = [ u, dr, lr, pt, bt]\n",
    "                            least_val_loss_accuracy = val_acc\n",
    "                            least_val_loss_model = model\n",
    "                            least_val_loss_history = history\n",
    "                            \n",
    "                        duration = time.time() - start_time\n",
    "                        time_past += duration\n",
    "                        avg_duration = time_past / i\n",
    "                        eta = time_past + avg_duration * (max - i)\n",
    "                        \n",
    "                        print(f\"\\n[eta: {to_hh_mm_ss(time_past)}/{to_hh_mm_ss(eta)}] [prev: {to_hh_mm_ss(duration)}] [avg: {to_hh_mm_ss(avg_duration)}]\")\n",
    "                            \n",
    "                        i += 1\n",
    "    \n",
    "    u, dr, lr, pt, bt = least_val_loss_params\n",
    "    print(\"\\nLeast validation loss:\")              \n",
    "    print(f\"\\tParams:\\t {{Units: {u}; Dropout rate: {dr}; Learning rate: {lr}; Patience: {pt}; Batch size: {bt}}}\")\n",
    "    print(\"\\tLoss:\\t\", least_val_loss)\n",
    "    print(\"\\tAccuracy:\\t\", least_val_loss_accuracy)\n",
    "    \n",
    "    best_model_file_name = f\"{VERSION}_Adam_{u}_{dr}_{lr}_{epochs}_{pt}_{bt}_{least_val_loss}.shallow_fnn.keras\"\n",
    "    os.rename(\n",
    "        os.path.join(MODELS_PATH, least_val_lost_file_name), \n",
    "        os.path.join(MODELS_PATH, best_model_file_name))\n",
    "    print(f\"\\nModel has been saved as '{best_model_file_name}'\")\n",
    "    \n",
    "    plot_history(least_val_loss_history, num_classes)\n",
    "    \n",
    "    print_results(least_val_loss_model)\n",
    "    \n",
    "    return least_val_loss_model, least_val_loss_history, num_classes\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model, history, _ = tune_model(\n",
    "    df=df,\n",
    "    units=[512, 768, 1024],\n",
    "    dropout_rates=[0],\n",
    "    learning_rates=[\n",
    "        0.0015, \n",
    "        0.00175, \n",
    "        lrs.FactorScheduler(factor=0.995, stop_factor=0.00075, base_lr=0.002), \n",
    "        lrs.FactorScheduler(factor=1.005, stop_factor=0.002, base_lr=0.00075)],\n",
    "    patience=[10, 20, 25, 30], \n",
    "    batch_sizes=[32, 64, 128])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b08d39c0a5a3d8dd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7d03e693bf977d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tune_hyperparameters(df,\n",
    "                         learning_rates,\n",
    "                         epsilons = [1e-07],\n",
    "                         beta_1s = [0.9],\n",
    "                         beta_2s = [0.999],\n",
    "                         weight_decay = [None],\n",
    "                         clipnorm = [None],\n",
    "                         clipvalue = [None],\n",
    "                         patience=[10, 20]):\n",
    "    import time\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    least_val_lost_file_name = f\"tuning_least_val_loss.fnn.keras\"\n",
    "    \n",
    "    to_hh_mm_ss = lambda seconds: str(timedelta(seconds=seconds)).rsplit(\".\")[0]\n",
    "    \n",
    "    least_val_loss = float('inf')\n",
    "    least_val_loss_params = []\n",
    "    least_val_loss_accuracy = float('inf')\n",
    "    least_val_loss_model = None  \n",
    "    least_val_loss_history = None\n",
    "    eta = None\n",
    "    \n",
    "    time_past = 0\n",
    "    \n",
    "    i = 1 \n",
    "    max = len(epsilons) * len(beta_1s) * len(beta_2s) * len(learning_rates) * len(weight_decay) * len(clipnorm) * len(clipvalue) * len(patience)\n",
    "    \n",
    "    print(\"[prev: N/A] [eta: TBD]\")\n",
    "    \n",
    "    for e in epsilons:\n",
    "        for b1 in beta_1s:\n",
    "            for b2 in beta_2s: \n",
    "                for lr in learning_rates: \n",
    "                    for wd in weight_decay:\n",
    "                        for cn in clipnorm:\n",
    "                            for cv in clipvalue:\n",
    "                                for pt in patience:\n",
    "                                    start_time = time.time()\n",
    "            \n",
    "                                    print(f\"[{i}/{max}] Epsilons: {e}; Beta 1: {b1}; Beta 2: {b2}; Learning rate: {lr}; Weight decay: {wd}; Clipnorm: {cn}; Clipvalue: {cv}; Patience: {pt}\")\n",
    "                                    \n",
    "                                    # TODO: add batch_size param to tune\n",
    "                                    model, history, num_classes = train_shallow_fnn_model(\n",
    "                                        df, \n",
    "                                        epochs=1000, \n",
    "                                        patience=pt, \n",
    "                                        units=1024,\n",
    "                                        dropout_rate=0,\n",
    "                                        learning_rate=lr,\n",
    "                                        epsilon = e,\n",
    "                                        beta_1 = b1,\n",
    "                                        beta_2 = b2,\n",
    "                                        weight_decay = wd,\n",
    "                                        clipnorm = cn,\n",
    "                                        clipvalue = cv,\n",
    "                                        batch_size=32,\n",
    "                                        verbose=0,\n",
    "                                        disable_save=True,\n",
    "                                        disable_plot_history=True,\n",
    "                                        disable_print_report=True)\n",
    "                                    \n",
    "                                    val_loss, val_acc = model.evaluate(x_test, test_labels)\n",
    "                                    print(f\"Loss: {val_loss}; Accuracy: {val_acc};\")\n",
    "                                    if val_loss < least_val_loss:\n",
    "                                        model.save(os.path.join(MODELS_PATH, least_val_lost_file_name))\n",
    "                                        least_val_loss = val_loss\n",
    "                                        least_val_loss_params = [e, b1, b2, lr, wd, cn, cv, pt]\n",
    "                                        least_val_loss_accuracy = val_acc\n",
    "                                        least_val_loss_model = model\n",
    "                                        least_val_loss_history = history\n",
    "                                        \n",
    "                                    duration = time.time() - start_time\n",
    "                                    time_past += duration\n",
    "                                    avg_duration = time_past / i\n",
    "                                    eta = time_past + avg_duration * (max - i)\n",
    "                                    \n",
    "                                    print(f\"\\n[eta: {to_hh_mm_ss(time_past)}/{to_hh_mm_ss(eta)}] [prev: {to_hh_mm_ss(duration)}] [avg: {to_hh_mm_ss(avg_duration)}]\")\n",
    "                                        \n",
    "                                    i += 1\n",
    "    \n",
    "    e, b1, b2, lr, wd, cn, cv, pt = least_val_loss_params\n",
    "    print(\"\\nLeast validation loss:\")              \n",
    "    print(f\"\\tParams:\\t {{Epsilons: {e}; Beta 1: {b1}; Beta 2: {b2}; Learning rate: {lr}; Weight decay: {wd}; Clipnorm: {cn}; Clipvalue: {cv}; Patience: {pt}}}\")\n",
    "    print(\"\\tLoss:\\t\", least_val_loss)\n",
    "    print(\"\\tAccuracy:\\t\", least_val_loss_accuracy)\n",
    "    \n",
    "    best_model_file_name = f\"{VERSION}_Adam_hyper_{e}_{b1}_{b2}_{lr}_{wd}_{cn}_{cv}_{pt}_{least_val_loss}.shallow_fnn.keras\"\n",
    "    os.rename(\n",
    "        os.path.join(MODELS_PATH, least_val_lost_file_name), \n",
    "        os.path.join(MODELS_PATH, best_model_file_name))\n",
    "    print(f\"\\nModel has been saved as '{best_model_file_name}'\")\n",
    "    \n",
    "    plot_history(least_val_loss_history, num_classes)\n",
    "    \n",
    "    print_results(least_val_loss_model)\n",
    "    \n",
    "    return least_val_loss_model, least_val_loss_history, num_classes\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78b70756c00f21ca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model, history, _ = tune_hyperparameters(\n",
    "    df=df,\n",
    "    learning_rates=[lrs.FactorScheduler(factor=0.995, stop_factor=0.00075, base_lr=0.002), ], \n",
    "    epsilons=[1e-06, 1e-07, 1e-08],\n",
    "    beta_1s=[0.4, 0.8, 0.9], # CANNOT BE EQUAL OR HIGHER THAN 1\n",
    "    beta_2s=[0.9, 0.999], # CANNOT BE EQUAL OR HIGHER THAN 1\n",
    "    weight_decay=[None, 0.01],\n",
    "    clipnorm=[None],\n",
    "    clipvalue=[None],\n",
    "    patience=[25]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33d917eb98bf4f9b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Manual training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d8b3f0d6e60b4d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model, _, _ = train_shallow_fnn_model(\n",
    "                        df, \n",
    "                        epochs=2000, \n",
    "                        patience=25, \n",
    "                        units=1024,\n",
    "                        dropout_rate=0,\n",
    "                        learning_rate=lrs.FactorScheduler(factor=0.995, stop_factor=0.00075, base_lr=0.002),\n",
    "                        verbose=2,\n",
    "                        epsilon=1e-07,\n",
    "                        beta_1=0.9,\n",
    "                        beta_2=0.999,\n",
    "                        weight_decay=0,\n",
    "                        clipnorm=None,\n",
    "                        clipvalue=None,\n",
    "                        disable_print_report=True,\n",
    "                        disable_save=True)\n",
    "\n",
    "_, _, y_pred = print_results(model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5511bb76b7347f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test (y-pred) difference plotting"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e6ccf30310a93a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "distribution = get_distribution(test_df, y_pred)\n",
    "# print(distribution)\n",
    "plot_distribution(distribution)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "284d312c99d33fff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def test():\n",
    "#     import os\n",
    "#     result = test_df\n",
    "#     result[\"predicted_country_risk\"] = model.predict(x_test).argmax(axis=1)\n",
    "# \n",
    "#     incorrectly_predicted = result[result[\"country_risk\"] != result[\"predicted_country_risk\"]]\n",
    "# \n",
    "#     m_df = pd.read_excel(MERGED_DATASET_PATH)\n",
    "#     wm_df = m_df.iloc[incorrectly_predicted.index, ]\n",
    "# \n",
    "#     wm_df[\"country_risk\"] = map_label_to_name(incorrectly_predicted[\"country_risk\"])\n",
    "#     wm_df[\"predicted_country_risk\"] = map_label_to_name(incorrectly_predicted[\"predicted_country_risk\"])\n",
    "# \n",
    "#     from configs.enums import RISKCLASSIFICATIONS\n",
    "# \n",
    "#     classifications = RISKCLASSIFICATIONS.get_attributes()\n",
    "#     def percentage_from_boundary(country_risk: str, risk_score: float) -> float:\n",
    "#         classification = classifications[country_risk]\n",
    "#         lower_percentage = abs(risk_score / classification.lower_bound - 1)\n",
    "#         upper_percentage =  abs(risk_score / classification.upper_bound - 1)\n",
    "#         return min(lower_percentage, upper_percentage)\n",
    "# \n",
    "#     wm_df[\"percentage_from_boundary\"] = [percentage_from_boundary(cr, nr) for cr, nr in zip(wm_df[\"country_risk\"], wm_df[\"norm_risk\"])]\n",
    "# \n",
    "#     cols = [\"year\", \"country\"] + list(incorrectly_predicted.columns) + [\"norm_risk\", \"percentage_from_boundary\"]\n",
    "#     wm_df = wm_df[cols]\n",
    "#     wm_df[\"within_2.5%\"] = [abs(x) <= 0.025 for x in wm_df[\"percentage_from_boundary\"]]\n",
    "#     return wm_df\n",
    "# \n",
    "# t = test()\n",
    "# t[\"percentage_from_boundary\"].describe()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d867b7eb7dd7f007"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Output .xlsx of incorrectly predicted rows"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c6b8265203f99e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from machine_learning.utils import output_incorrectly_predicted_xlsx\n",
    "output_incorrectly_predicted_xlsx(test_df, y_pred, \"shallow_fnn\")  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac84db8a422016c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Shap; Feature importance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5711f7e08f44985b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, x_train)\n",
    "shap_values = explainer.shap_values(shap.sample(x_test, 20), nsamples=100, random_state=41) # default of nsamples = 2 * X.shape[1] + 2048 = 2066 \n",
    "# explainer.save()\n",
    "\n",
    "from configs.enums import RISKCLASSIFICATIONS\n",
    "feature_names = df.columns.tolist()\n",
    "feature_names.remove(Column.COUNTRY_RISK)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d439d16505c7c53c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, x_test, \n",
    "                  feature_names=feature_names,\n",
    "                  class_names=RISKCLASSIFICATIONS.get_names())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf0af0cca82680a9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, x_test, \n",
    "                  feature_names=feature_names,\n",
    "                  class_names=RISKCLASSIFICATIONS.get_names(), plot_size=(20, 10))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b4286323a2e555e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "186c68a887798851"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
